# üöÄ TodoApp - AutoScaling en Google Cloud con Ansible y Kubernetes

## üìã Tabla de Contenidos

- [Descripci√≥n General](#descripci√≥n-general)
- [Arquitectura](#arquitectura)
- [Requisitos Previos](#requisitos-previos)
- [Configuraci√≥n de AutoScaling](#configuraci√≥n-de-autoscaling)
- [Instalaci√≥n y Despliegue](#instalaci√≥n-y-despliegue)
- [Pruebas de AutoScaling](#pruebas-de-autoscaling)
- [Monitoreo](#monitoreo)
- [Limpieza de Recursos](#limpieza-de-recursos)
- [Troubleshooting](#troubleshooting)

---

## üéØ Descripci√≥n General

Este proyecto implementa **AutoScaling completo** tanto a nivel de **pods** (Horizontal Pod Autoscaler) como de **nodos** (Cluster Autoscaler) en **Google Kubernetes Engine (GKE)** usando **Ansible** como herramienta de Infrastructure as Code (IaC).

### Caracter√≠sticas de AutoScaling

| Tipo | Componente | Configuraci√≥n |
|------|------------|---------------|
| **Pod Autoscaling** | Backend | 2-10 r√©plicas (CPU: 50%, Mem: 70%) |
| **Pod Autoscaling** | Frontend | 2-8 r√©plicas (CPU: 60%, Mem: 75%) |
| **Node Autoscaling** | Cluster GKE | 2-10 nodos (e2-standard-2) |

### ‚ú® Caracter√≠sticas Principales

- ‚úÖ **Cluster GKE** con Cluster Autoscaler habilitado
- ‚úÖ **HPA (Horizontal Pod Autoscaler)** para backend y frontend
- ‚úÖ **M√©tricas de CPU y Memoria** para autoscaling
- ‚úÖ **Pol√≠ticas de escalado** optimizadas (scale-up r√°pido, scale-down gradual)
- ‚úÖ **Deployment automatizado** con Ansible
- ‚úÖ **Scripts de prueba de carga** incluidos
- ‚úÖ **Monitoreo en tiempo real** de m√©tricas y escalado

---

## üèóÔ∏è Arquitectura

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     Google Cloud Platform                    ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ              GKE Cluster (Autoscaling)                ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                                                         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ          Node Pool (2-10 nodes)              ‚îÇ     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ                                                ‚îÇ     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ  Backend Pods  ‚îÇ  ‚îÇ Frontend Pods  ‚îÇ     ‚îÇ     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ   (HPA 2-10)   ‚îÇ  ‚îÇ   (HPA 2-8)    ‚îÇ     ‚îÇ     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ                ‚îÇ  ‚îÇ                ‚îÇ     ‚îÇ     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ  CPU: 50%      ‚îÇ  ‚îÇ  CPU: 60%      ‚îÇ     ‚îÇ     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ  Mem: 70%      ‚îÇ  ‚îÇ  Mem: 75%      ‚îÇ     ‚îÇ     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ                                                ‚îÇ     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                          ‚îÇ     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ   PostgreSQL   ‚îÇ                          ‚îÇ     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ   (1 replica)  ‚îÇ                          ‚îÇ     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                          ‚îÇ     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                                                         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ           Metrics Server                      ‚îÇ     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ         (HPA Data Source)                     ‚îÇ     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ         Container Registry (GCR)                      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   - todoapp-backend:latest                            ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   - todoapp-frontend:latest                           ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Flujo de AutoScaling

```
1. Carga Alta ‚Üí M√©tricas Aumentan (CPU/Mem)
                    ‚Üì
2. Metrics Server ‚Üí Recolecta m√©tricas cada 15s
                    ‚Üì
3. HPA Controller ‚Üí Detecta threshold excedido
                    ‚Üì
4. Scale-Up Pods ‚Üí Crea nuevos pods (30s scale-up)
                    ‚Üì
5. Si no hay recursos ‚Üí Cluster Autoscaler activa
                    ‚Üì
6. Nuevos Nodos ‚Üí GKE provisiona nodos adicionales
                    ‚Üì
7. Pods Programados ‚Üí Nuevos pods en nuevos nodos
                    ‚Üì
8. Carga Baja ‚Üí Scale-Down gradual (5min stabilization)
```

---

## üìã Requisitos Previos

### 1. Software Necesario

```bash
# Ansible
sudo apt update
sudo apt install -y ansible

# Google Cloud SDK
curl https://sdk.cloud.google.com | bash
exec -l $SHELL
gcloud init

# kubectl
gcloud components install kubectl

# Helm
curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash

# Docker (para build de im√°genes)
sudo apt install -y docker.io
sudo usermod -aG docker $USER
```

### 2. Cuenta de GCP

1. **Crear proyecto en GCP**:
   ```bash
   export GCP_PROJECT_ID="tu-proyecto-id"
   gcloud projects create $GCP_PROJECT_ID
   gcloud config set project $GCP_PROJECT_ID
   ```

2. **Habilitar facturaci√≥n**:
   - Ve a: https://console.cloud.google.com/billing
   - Vincula el proyecto con una cuenta de facturaci√≥n

3. **Crear Service Account**:
   ```bash
   # Crear service account
   gcloud iam service-accounts create todoapp-deployer \
       --display-name="TodoApp Deployer"
   
   # Asignar roles necesarios
   gcloud projects add-iam-policy-binding $GCP_PROJECT_ID \
       --member="serviceAccount:todoapp-deployer@${GCP_PROJECT_ID}.iam.gserviceaccount.com" \
       --role="roles/container.admin"
   
   gcloud projects add-iam-policy-binding $GCP_PROJECT_ID \
       --member="serviceAccount:todoapp-deployer@${GCP_PROJECT_ID}.iam.gserviceaccount.com" \
       --role="roles/compute.admin"
   
   gcloud projects add-iam-policy-binding $GCP_PROJECT_ID \
       --member="serviceAccount:todoapp-deployer@${GCP_PROJECT_ID}.iam.gserviceaccount.com" \
       --role="roles/storage.admin"
   
   # Crear y descargar key
   mkdir -p ~/.gcp
   gcloud iam service-accounts keys create ~/.gcp/credentials.json \
       --iam-account=todoapp-deployer@${GCP_PROJECT_ID}.iam.gserviceaccount.com
   ```

4. **Configurar variables de entorno**:
   ```bash
   export GCP_PROJECT_ID="tu-proyecto-id"
   export GCP_CREDENTIALS_FILE="$HOME/.gcp/credentials.json"
   
   # Agregar a ~/.bashrc para persistencia
   echo "export GCP_PROJECT_ID=\"$GCP_PROJECT_ID\"" >> ~/.bashrc
   echo "export GCP_CREDENTIALS_FILE=\"$HOME/.gcp/credentials.json\"" >> ~/.bashrc
   ```

### 3. Cuotas de GCP

Verifica que tienes cuotas suficientes:
- **CPUs**: M√≠nimo 20 vCPUs en us-central1
- **IP Addresses**: M√≠nimo 10 IPs
- **Persistent Disk**: 500 GB

```bash
# Ver cuotas actuales
gcloud compute project-info describe --project=$GCP_PROJECT_ID
```

---

## ‚öôÔ∏è Configuraci√≥n de AutoScaling

### Configuraci√≥n de Variables

Edita `ansible/inventories/gcp/group_vars/all.yml`:

```yaml
# Proyecto GCP
gcp_project_id: "tu-proyecto-id"
gcp_region: "us-central1"
gcp_zone: "us-central1-a"

# Cluster Configuration
gke_cluster_name: "todoapp-autoscaling-cluster"

# Node Pool (Cluster Autoscaler)
gke_node_pool:
  name: "default-pool"
  initial_node_count: 2        # Nodos iniciales
  min_node_count: 2            # M√≠nimo de nodos
  max_node_count: 10           # M√°ximo de nodos
  machine_type: "e2-standard-2" # 2 vCPUs, 8GB RAM
  
# HPA Configuration - Backend
autoscaling:
  backend:
    min_replicas: 2
    max_replicas: 10
    target_cpu_utilization: 50      # Scale cuando CPU > 50%
    target_memory_utilization: 70   # Scale cuando Mem > 70%
    
  # HPA Configuration - Frontend
  frontend:
    min_replicas: 2
    max_replicas: 8
    target_cpu_utilization: 60      # Scale cuando CPU > 60%
    target_memory_utilization: 75   # Scale cuando Mem > 75%
```

### Pol√≠ticas de Escalado

Las pol√≠ticas est√°n configuradas en `helm/todoapp/templates/hpa.yaml`:

**Scale-Up (R√°pido)**:
- Sin estabilizaci√≥n (0 segundos)
- M√°ximo: 100% o 4 pods cada 30 segundos
- Pol√≠tica: Tomar el m√°ximo

**Scale-Down (Gradual)**:
- Estabilizaci√≥n: 5 minutos
- M√°ximo: 50% o 2 pods cada 60 segundos
- Pol√≠tica: Tomar el m√≠nimo

---

## üöÄ Instalaci√≥n y Despliegue

### M√©todo 1: Despliegue Completo (Recomendado)

```bash
cd ansible

# Ejecutar el playbook principal
ansible-playbook main.yml

# Este playbook ejecuta autom√°ticamente:
# 1. Creaci√≥n del cluster GKE
# 2. Build y push de im√°genes a GCR
# 3. Despliegue de la aplicaci√≥n con Helm
```

### M√©todo 2: Paso a Paso

```bash
cd ansible

# Paso 1: Crear cluster GKE
ansible-playbook setup-gke-cluster.yml

# Paso 2: Build y push de im√°genes
ansible-playbook build-and-push-images.yml

# Paso 3: Desplegar aplicaci√≥n
ansible-playbook deploy-app.yml
```

### M√©todo 3: Tags Espec√≠ficos

```bash
# Solo crear cluster
ansible-playbook main.yml --tags cluster

# Solo build de im√°genes
ansible-playbook main.yml --tags build

# Solo deployment
ansible-playbook main.yml --tags deploy
```

### Verificar Despliegue

```bash
# Verificar cluster
kubectl cluster-info
kubectl get nodes

# Verificar aplicaci√≥n
kubectl get all -n todoapp

# Verificar HPA
kubectl get hpa -n todoapp

# Obtener URL de acceso
kubectl get svc todoapp-frontend -n todoapp
```

---

## üß™ Pruebas de AutoScaling

### 1. Monitoreo en Tiempo Real

```bash
cd load-testing

# Iniciar monitor (en una terminal separada)
./monitor-autoscaling.sh
```

Este script muestra:
- Estado de HPAs (targets, replicas)
- Pods actuales y su distribuci√≥n
- M√©tricas de CPU/Memoria
- Estado de nodos
- Eventos de escalado recientes

### 2. Prueba de Carga B√°sica

```bash
# Generar carga moderada (5 minutos)
./simple-load-test.sh

# Con configuraci√≥n personalizada
CONCURRENT_WORKERS=20 DURATION=600 ./simple-load-test.sh
```

**Resultado esperado**:
- Backend escala de 2 a 4-6 pods
- Frontend puede escalar ligeramente
- Nodos se mantienen en 2-3

### 3. Prueba de Carga Avanzada

```bash
# Test con monitoreo integrado
./run-load-test.sh

# Configuraci√≥n personalizada
CONCURRENT_REQUESTS=100 TOTAL_REQUESTS=10000 DURATION=600 ./run-load-test.sh
```

**Resultado esperado**:
- Backend escala hacia 8-10 pods
- Frontend escala a 4-6 pods
- Cluster puede agregar 1-2 nodos nuevos

### 4. Prueba Extrema (Escalado de Nodos)

‚ö†Ô∏è **ADVERTENCIA**: Esta prueba generar√° costos significativos en GCP.

```bash
# Generar carga extrema
./extreme-load-test.sh

# Esto crear√° 20 pods generadores de carga
```

**Resultado esperado**:
- Backend escala a m√°ximo (10 pods)
- Frontend escala a m√°ximo (8 pods)
- Cluster escala a 5-8 nodos
- **Costos**: ~$2-5 USD durante la prueba

### Detener Pruebas de Carga

```bash
# Detener generadores de carga
kubectl delete pods -n todoapp -l run=load-generator

# Limpiar recursos
kubectl delete pod load-generator -n todoapp --ignore-not-found
```

---

## üìä Monitoreo

### Comandos de Monitoreo √ötiles

```bash
# Ver HPAs en tiempo real
kubectl get hpa -n todoapp -w

# Ver pods y su uso de recursos
kubectl top pods -n todoapp

# Ver nodos y su uso
kubectl top nodes

# Ver eventos de escalado
kubectl get events -n todoapp --sort-by='.lastTimestamp' | grep -i scale

# Describe HPA para detalles
kubectl describe hpa -n todoapp

# Ver logs de pods espec√≠ficos
kubectl logs -n todoapp -l app.kubernetes.io/component=backend --tail=100

# Ver distribuci√≥n de pods en nodos
kubectl get pods -n todoapp -o wide
```

### M√©tricas Clave

| M√©trica | Comando | Threshold |
|---------|---------|-----------|
| CPU Backend | `kubectl top pods -n todoapp -l component=backend` | > 50% ‚Üí Scale Up |
| Mem Backend | `kubectl top pods -n todoapp -l component=backend` | > 70% ‚Üí Scale Up |
| CPU Frontend | `kubectl top pods -n todoapp -l component=frontend` | > 60% ‚Üí Scale Up |
| Replicas Backend | `kubectl get hpa -n todoapp` | 2-10 pods |
| Replicas Frontend | `kubectl get hpa -n todoapp` | 2-8 pods |
| Nodos Cluster | `kubectl get nodes` | 2-10 nodos |

### Dashboard de M√©tricas (GCP Console)

1. Ve a: https://console.cloud.google.com/kubernetes/clusters
2. Selecciona tu cluster ‚Üí "Workloads"
3. Observa:
   - CPU y Memoria por pod
   - Distribuci√≥n de pods
   - Eventos de autoscaling
   - Utilizaci√≥n de nodos

---

## üßπ Limpieza de Recursos

### Opci√≥n 1: Usando Ansible (Recomendado)

```bash
cd ansible
ansible-playbook cleanup.yml
```

Este playbook:
1. Solicita confirmaci√≥n
2. Elimina el release de Helm
3. Elimina el namespace
4. Elimina el cluster GKE
5. Elimina la subnet
6. Elimina la VPC network

### Opci√≥n 2: Manual

```bash
# Eliminar aplicaci√≥n
helm uninstall todoapp -n todoapp
kubectl delete namespace todoapp

# Eliminar cluster GKE
gcloud container clusters delete todoapp-autoscaling-cluster \
    --zone=us-central1-a \
    --project=$GCP_PROJECT_ID \
    --quiet

# Eliminar red
gcloud compute networks subnets delete todoapp-subnet \
    --region=us-central1 \
    --project=$GCP_PROJECT_ID \
    --quiet

gcloud compute networks delete todoapp-network \
    --project=$GCP_PROJECT_ID \
    --quiet

# Eliminar im√°genes de GCR (opcional)
gcloud container images delete gcr.io/$GCP_PROJECT_ID/todoapp-backend:latest --quiet
gcloud container images delete gcr.io/$GCP_PROJECT_ID/todoapp-frontend:latest --quiet
```

### Verificar Limpieza

```bash
# Verificar que no hay clusters
gcloud container clusters list --project=$GCP_PROJECT_ID

# Verificar que no hay recursos de red
gcloud compute networks list --project=$GCP_PROJECT_ID
```

---

## üîß Troubleshooting

### Problema: HPA no escala

**S√≠ntoma**: HPA muestra `<unknown>` en targets

```bash
kubectl get hpa -n todoapp
# NAME                REFERENCE                      TARGETS         MINPODS   MAXPODS
# todoapp-backend     Deployment/todoapp-backend     <unknown>/50%   2         10
```

**Soluciones**:

1. Verificar metrics-server:
   ```bash
   kubectl get deployment metrics-server -n kube-system
   kubectl logs -n kube-system -l k8s-app=metrics-server
   ```

2. Reinstalar metrics-server:
   ```bash
   helm upgrade --install metrics-server metrics-server/metrics-server \
       --namespace kube-system \
       --set args[0]="--kubelet-insecure-tls" \
       --set args[1]="--kubelet-preferred-address-types=InternalIP"
   ```

3. Esperar 2-3 minutos para que se recolecten m√©tricas

### Problema: Pods no tienen suficientes recursos

**S√≠ntoma**: Pods en estado `Pending` o `CrashLoopBackOff`

```bash
kubectl describe pod <pod-name> -n todoapp
```

**Soluciones**:

1. Verificar recursos del nodo:
   ```bash
   kubectl describe nodes
   ```

2. Aumentar l√≠mites de recursos en `values.yaml`

3. Forzar escalado de nodos:
   ```bash
   # El cluster deber√≠a escalar autom√°ticamente
   # Si no, verifica los logs del cluster autoscaler
   ```

### Problema: Cluster Autoscaler no a√±ade nodos

**S√≠ntoma**: Pods `Pending` pero sin nodos nuevos

**Soluciones**:

1. Verificar que el autoscaling est√° habilitado:
   ```bash
   gcloud container clusters describe todoapp-autoscaling-cluster \
       --zone=us-central1-a \
       --format="value(autoscaling)"
   ```

2. Verificar cuotas de GCP:
   ```bash
   gcloud compute project-info describe --project=$GCP_PROJECT_ID
   ```

3. Ver logs del cluster autoscaler:
   ```bash
   kubectl logs -n kube-system -l k8s-app=cluster-autoscaler
   ```

### Problema: LoadBalancer no obtiene IP externa

**S√≠ntoma**: `EXTERNAL-IP` permanece en `<pending>`

```bash
kubectl get svc todoapp-frontend -n todoapp
```

**Soluciones**:

1. Esperar 2-3 minutos (puede tomar tiempo)

2. Verificar cuotas de IPs:
   ```bash
   gcloud compute addresses list --project=$GCP_PROJECT_ID
   ```

3. Describir el servicio:
   ```bash
   kubectl describe svc todoapp-frontend -n todoapp
   ```

### Problema: Ansible playbook falla en autenticaci√≥n

**S√≠ntoma**: Error `Could not authenticate`

**Soluciones**:

1. Verificar variables de entorno:
   ```bash
   echo $GCP_PROJECT_ID
   echo $GCP_CREDENTIALS_FILE
   ```

2. Verificar archivo de credenciales:
   ```bash
   test -f $GCP_CREDENTIALS_FILE && echo "OK" || echo "MISSING"
   ```

3. Re-autenticar:
   ```bash
   gcloud auth activate-service-account --key-file=$GCP_CREDENTIALS_FILE
   gcloud config set project $GCP_PROJECT_ID
   ```

### Problema: Im√°genes no se pueden pull

**S√≠ntoma**: `ImagePullBackOff` o `ErrImagePull`

**Soluciones**:

1. Verificar que las im√°genes existen en GCR:
   ```bash
   gcloud container images list --repository=gcr.io/$GCP_PROJECT_ID
   ```

2. Verificar permisos:
   ```bash
   gcloud projects get-iam-policy $GCP_PROJECT_ID
   ```

3. Re-build y push:
   ```bash
   cd ansible
   ansible-playbook build-and-push-images.yml
   ```

---

## üìä Costos Estimados

### Configuraci√≥n Base (2 nodos)

| Recurso | Cantidad | Costo/hora | Costo/d√≠a | Costo/mes |
|---------|----------|------------|-----------|-----------|
| e2-standard-2 | 2 nodos | $0.134 | $3.22 | $96.60 |
| Persistent Disk (50GB) | 2 discos | $0.008 | $0.19 | $5.70 |
| LoadBalancer | 1 | $0.025 | $0.60 | $18.00 |
| **TOTAL** | - | **~$0.35** | **~$8.40** | **~$252** |

### Durante Autoscaling Extremo (10 nodos)

| Recurso | Cantidad | Costo/hora | Costo/d√≠a |
|---------|----------|------------|-----------|
| e2-standard-2 | 10 nodos | $0.670 | $16.08 |
| Persistent Disk (50GB) | 10 discos | $0.040 | $0.96 |
| LoadBalancer | 1 | $0.025 | $0.60 |
| **TOTAL** | - | **~$1.75** | **~$42** |

‚ö†Ô∏è **Recomendaciones**:
- Ejecuta pruebas de carga por per√≠odos cortos
- Limpia recursos inmediatamente despu√©s de las pruebas
- Configura alertas de presupuesto en GCP
- Considera usar nodos preemptible para reducir costos

---

## üìö Referencias

- [GKE Cluster Autoscaler](https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-autoscaler)
- [Kubernetes HPA](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/)
- [Metrics Server](https://github.com/kubernetes-sigs/metrics-server)
- [Ansible GCP Modules](https://docs.ansible.com/ansible/latest/collections/google/cloud/)
- [Helm Documentation](https://helm.sh/docs/)

---

## üéì Conceptos Clave

### Horizontal Pod Autoscaler (HPA)

- **Qu√© hace**: Escala el n√∫mero de pods bas√°ndose en m√©tricas
- **M√©tricas soportadas**: CPU, Memoria, Custom Metrics
- **Evaluaci√≥n**: Cada 15 segundos por defecto
- **Algoritmo**: `desiredReplicas = ceil[currentReplicas * (currentMetric / targetMetric)]`

### Cluster Autoscaler

- **Qu√© hace**: Escala el n√∫mero de nodos en el cluster
- **Cu√°ndo escala UP**: Cuando hay pods `Pending` por falta de recursos
- **Cu√°ndo escala DOWN**: Cuando nodos est√°n sub-utilizados (< 50%) por > 10 minutos
- **Protecciones**: No elimina nodos con pods que no pueden ser reprogramados

### Metrics Server

- **Funci√≥n**: Recolecta m√©tricas de recursos (CPU/Mem) de kubelet
- **Frecuencia**: Cada 60 segundos
- **Almacenamiento**: In-memory (no persistente)
- **Clientes**: HPA, kubectl top, VPA

---

## ‚úÖ Checklist de Validaci√≥n

Usa este checklist para verificar que todo funciona correctamente:

- [ ] Cluster GKE creado y accesible
- [ ] Metrics-server instalado y funcionando
- [ ] HPAs creados y mostrando m√©tricas v√°lidas
- [ ] Pods backend y frontend corriendo
- [ ] LoadBalancer tiene IP externa
- [ ] Aplicaci√≥n accesible desde navegador
- [ ] HPA escala pods bajo carga
- [ ] Cluster Autoscaler a√±ade nodos cuando es necesario
- [ ] Scale-down funciona despu√©s de reducir carga
- [ ] Monitoreo muestra m√©tricas en tiempo real

---

## ü§ù Contribuciones

Para mejoras o reportar issues:
1. Documenta el problema con logs y comandos ejecutados
2. Incluye la configuraci√≥n de `group_vars/all.yml`
3. Especifica la versi√≥n de GKE, kubectl, Helm y Ansible

---

## üìÑ Licencia

Este proyecto es parte de un ejercicio acad√©mico de Cloud Computing.

---

**¬°Importante!** üî¥ No olvides ejecutar `ansible-playbook cleanup.yml` cuando termines para evitar cargos innecesarios en tu cuenta de GCP.
